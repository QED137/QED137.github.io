<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Statistics Blog</title>
    <link rel="stylesheet" href="statistics.css">
    <link href="https://fonts.googleapis.com/css2?family=Dancing+Script&display=swap" rel="stylesheet">
    <link href="https://fonts.cdnfonts.com/css/kapakana" rel="stylesheet">
                
</head>
<body>

    <!-- Navbar -->
    <header class="navbar">
        <h1>Statistics & Data Science</h1>
        <nav>
            <a href="../index.html">Home</a>
            <a href="blogs.html" class="active">Blogs</a>
            <!-- <a href="statistics.html" class="active">Statistics</a>
            <a href="physics.html">Physics</a>
            <a href="mathematics.html">Mathematics</a>
            <a href="machinelearning.html">Machine Learning</a> -->
            <a href="../gallery.html">Gallery</a>
            <a href="../about/about.html">About</a>
        </nav>
    </header>
    
    <!-- Hero Section -->
    <!-- <section class="hero">
        <h2>Unraveling Insights from Data</h2>
        <p>Explore statistical methods, probability theories, and real-world data applications.</p>
    </section> -->
    <section class="blog-header">
        <div class="container">
            <h1>Automating Real-Time Data Pipelines: Integrating Weather and Flight APIs with Python and MySQL</h1>
            <p>Learn how to build a real-time data pipeline using Python to fetch weather and flight information and store it in a MySQL database.</p>  
            <p>By <strong>Janmajay Kumar</strong> | October 2024</p>
        </div>
    </section>

    <section class="blog-content">
        <div class="container">
            <h2>Introduction</h2>
            <p ><i>
                In this article, we'll walk you through the steps of building an automated data 
                pipeline that integrates data from OpenWeather API and AeroDataBox API to store 
                real-time weather and flight information into a MySQL database. Along the way, we'll 
                learn about the importance of data pipelines in ensuring smooth and scalable data processing 
                workflows and how to secure sensitive credentials such as API keys and database passwords.</i>
            </p>
            <h2>The Importance of Database Pipelines in Modern Data Engineering</h2>
            <p>In today’s data-driven world, businesses and organizations rely on vast amounts of 
                data to make informed decisions, optimize operations, and predict future trends.
                 This data comes from multiple sources: customer transactions, IoT devices, weather
                  APIs, flight information, and more. The challenge lies not only in collecting 
                this data but also in storing, processing, and analyzing it effectively.</p>

                <p>This is where <strong>database pipelines </strong>come into play. A well-structured 
                    database pipeline ensures data flows seamlessly from source to storage 
                    and then to analysis. It connects different systems and data sources,
                     ensuring the right information 
                    is available in the right format at the right time.</p>
                    
            <h2>What is a Database Pipeline?</h2>   
            <p>A <strong>database pipeline</strong> is a series of
                 steps that data goes through from collection to storage in a database, 
                and then further to processing or analysis. 
                The pipeline typically involves multiple stages: </p> 
                <ul   
            <h2>0. Architecture Overview</h2>
            <p>The real-time data pipeline consists of:</p>
            <ul>
                <li><strong>Google Pub/Sub</strong> - Event ingestion & messaging</li>
                <li><strong>Cloud Dataflow</strong> - Stream processing & transformations</li>
                <li><strong>BigQuery</strong> - Data storage & analytics</li>
                <li><strong>Cloud Functions</strong> - Event-driven triggers</li>
            </ul>
            <img src="../images/google-cloud-architecture.png" alt="Google Cloud Architecture">

            <h2>2. Setting Up Pub/Sub for Data Streaming</h2>
            <p>Google Pub/Sub acts as a messaging service, allowing multiple sources to publish data, which is then processed downstream.</p>
            <pre><code>
gcloud pubsub topics create my-topic
gcloud pubsub subscriptions create my-subscription --topic=my-topic
            </code></pre>

            <h2>3. Processing Data Using Cloud Dataflow</h2>
            <p>Google Cloud Dataflow, powered by Apache Beam, is used for stream processing.</p>
            <pre><code>
python my_dataflow_pipeline.py --runner=DataflowRunner --project=my-project
            </code></pre>

            <h2>4. Storing Data in BigQuery</h2>
            <p>Processed data is stored in BigQuery for analytics.</p>
            <pre><code>
bq query --use_legacy_sql=false 'SELECT * FROM my_dataset.realtime_data'
            </code></pre>

            <h2>5. Automating with Cloud Functions</h2>
            <p>Google Cloud Functions automate the pipeline by triggering actions when new data arrives.</p>
            <pre><code>
gcloud functions deploy my-function --trigger-topic=my-topic
            </code></pre>

            <h2>Conclusion</h2>
            <p>By integrating these GCP services, we create a scalable, automated, and real-time data pipeline. This architecture is ideal for applications requiring instant data processing and analytics.</p>
        </div>
    </section>

    <footer>
        <p>&copy; 2025 Data Engineering Blog | Created by Janmajay Kumar</p>
    </footer>

</body>
</html>