<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Statistics Blog</title>
    <link rel="stylesheet" href="statistics.css">
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=default'></script>

</head>
<body>

    <!-- Navbar -->
    <header class="navbar">
        <h1>Statistics & Data Science</h1>
        <nav>
            <a href="../index.html">Home</a>
            <a href="blogs.html" class="active">Blogs</a>
           <!-- <a href="statistics.html" class="active">Statistics</a>
            <a href="physics.html">Physics</a>
            <a href="mathematics.html">Mathematics</a>
            <a href="machinelearning.html">Machine Learning</a> -->
            <a href="../gallery.html">Gallery</a>
            <a href="../about/about.html">About</a>
        </nav>
    </header>
    
    <!-- Hero Section -->
    <!-- <section class="hero">
        <h2>Unraveling Insights from Data</h2>
        <p>Explore statistical methods, probability theories, and real-world data applications.</p>
    </section> -->
    <section class="blog-header">
        <div class="container">
            <h1>From Data to Decisions: A Beginner’s Guide to Statistical Inference</h1>
            <p>By <strong>Janmajay Kumar</strong> | October 2024</p>
        </div>
    </section>

    <section class="blog-content">
        <div class="container">
            <!-- <header id="title-block-header">
            <h1 class="title">From Data to Decisions: A Beginner’s Guide to
            Statistical Inference</h1>
            <p class="author">Janmajay Kumar</p>
            <p class="date">October 2024</p>
            </header> -->
            <h1 id="real-world-questions-needing-statistical-inference">Real-World
            Questions: Needing Statistical Inference</h1>
            <p>Statistical inference is essential when we need to make sense of
            real-world data. Often, we observe differences or trends, but how do we
            know whether those observations are meaningful or simply due to chance?
            Let’s look at five common real-life questions across different fields,
            each requiring statistical inference to find answers.</p>
            <ol>
            <li><p><strong>Social Science: Does Education Affect
            Income?</strong></p>
            <ul>
            <li><p><strong>Question</strong>: Do people with a college degree earn
            more than those without one?</p></li>
            <li><p><strong>Example</strong>: In a survey of 100 people, 50 college
            graduates have an average income of $60,000, while the average income
            for 50 people without a degree is $45,000.</p></li>
            <li><p><strong>Objective</strong>: We want to determine if the observed
            income difference is real or simply a result of random variation in the
            sample.</p></li>
            </ul></li>
            <li><p><strong>Business: Does Customer Happiness Drive
            Sales?</strong></p>
            <ul>
            <li><p><strong>Question</strong>: Do happier customers spend more
            money?</p></li>
            <li><p><strong>Example</strong>: Out of 200 surveyed customers, those
            who spent more than $500 rated their satisfaction at 8.5 out of 10,
            while those who spent less gave a rating of 7.2.</p></li>
            <li><p><strong>Objective</strong>: The goal is to find out whether
            customer satisfaction truly influences how much people spend, or if the
            difference in ratings is just coincidental.</p></li>
            </ul></li>
            <li><p><strong>Finance: Is the Stock Market More Unstable During
            Recessions?</strong></p>
            <ul>
            <li><p><strong>Question</strong>: Is the stock market more volatile
            during economic downturns?</p></li>
            <li><p><strong>Example</strong>: During a recession, daily stock price
            changes averaged 2.8%, while in stable economic times, the changes
            averaged 1.5%.</p></li>
            <li><p><strong>Objective</strong>: We want to figure out if the stock
            market genuinely becomes more unstable during recessions or if the
            observed volatility could just be random.</p></li>
            </ul></li>
            <li><p><strong>Physics: Does Gravity Vary in Different
            Locations?</strong></p>
            <ul>
            <li><p><strong>Question</strong>: Is the gravitational force slightly
            different at two different locations on Earth?</p></li>
            <li><p><strong>Example</strong>: At one location, gravity was measured
            10 times and averaged 9.81 m/s². At another location, it was measured at
            9.78 m/s².</p></li>
            <li><p><strong>Objective</strong>: We aim to see if this small
            difference in gravity is significant or just due to random measurement
            variation.</p></li>
            </ul></li>
            <li><p><strong>Biology: Is the New Drug Better for Lowering Blood
            Pressure?</strong></p>
            <p><strong>Question</strong>: Does a new drug reduce blood pressure more
            effectively than the current treatment?</p>
            <p><strong>Example</strong>: In a clinical trial, 50 patients taking the
            new drug had an average blood pressure drop of 12 mmHg, while 50
            patients on the existing drug experienced a 9 mmHg drop.</p>
            <p><strong>Objective</strong>: We’re interested in determining if the
            new drug is truly more effective at lowering blood pressure or if the
            observed difference is just a coincidence.</p></li>
            <li><p><strong>Sports: Does Practicing More Improve
            Performance?</strong></p>
            <p><strong>Question</strong>: Do athletes who practice more hours each
            week perform better in competitions?</p>
            <p><strong>Example</strong>: We tracked the weekly practice hours of 50
            soccer players. Players who practiced more than 10 hours per week scored
            an average of 15 goals in the season, while those practicing fewer than
            10 hours scored 10 goals on average. <strong>Objective</strong>: We want
            to determine if practicing more hours genuinely improves performance or
            if the difference in goal-scoring could have occurred by
            chance.</p></li>
            <li><p><strong>Sports: Does Team A Have a Home Advantage?</strong></p>
            <p><strong>Question</strong>: Do teams win more often when they play at
            home compared to playing away?</p>
            <p><strong>Example</strong>: Over a season, Team A won 70% of its home
            games but only 40% of its away games.</p>
            <p><strong>Objective</strong>: We’re interested in figuring out if the
            "home advantage" is real or if the difference in winning percentage
            could be just random variation.</p></li>
            </ol>
            <h1 id="introduction-what-is-statistical-inference">Introduction: What
            is Statistical Inference?</h1>
            <p>Statistical inference is the process by which we use data from a
            sample to make generalizations or conclusions about a larger population.
            The core idea is that, in many situations, it is impractical or
            impossible to collect data from every individual in a population.
            Instead, we gather data from a smaller, manageable subset of the
            population, known as a sample, and use that information to make educated
            guesses or inferences about the entire population.</p>
            <p>Statistical inference helps answer questions like: "Is this observed
            effect real, or could it have occurred by chance?" or "How confident can
            we be in the results from our sample?" Two primary tools used in
            statistical inference are estimation and hypothesis testing.</p>
            <p>Let’s look at two examples to clarify how statistical inference
            works:</p>
            <p>Example 1: Estimating Average Household Income</p>
            <p>Suppose a city wants to know the average household income of its
            residents. Instead of surveying every household, which could be
            expensive and time-consuming, they survey 500 randomly selected
            households. From this sample, they find that the average income is
            $60,000. But the question remains: does this sample accurately reflect
            the income of all households in the city?</p>
            <p>Statistical inference allows us to estimate the true average income
            of the city’s households based on this sample. By using techniques such
            as confidence intervals, we can say something like: "We are 95%
            confident that the true average household income in this city is between
            <span
            class="math inline">58, 000<em>a</em><em>n</em><em>d</em></span>62,000."</p>
            <p>Example 2: Testing the Effectiveness of a New Drug</p>
            <p>Imagine that a pharmaceutical company is testing a new drug that aims
            to reduce blood pressure. They conduct a clinical trial with 100
            participants: 50 take the new drug, and 50 take the standard treatment.
            After several weeks, they observe that the group taking the new drug has
            an average reduction in blood pressure of 12 mmHg, while the standard
            treatment group has an average reduction of 9 mmHg.</p>
            <p>Is this difference in blood pressure reduction (3 mmHg) meaningful?
            Or could it just be due to random chance? Statistical inference allows
            the researchers to test whether the observed difference is statistically
            significant. Through hypothesis testing, they can infer whether the new
            drug is likely to be more effective than the standard treatment for the
            population at large, or if the observed difference is too small to
            conclude anything definitively.</p>
            <h1 id="what-is-to-be-inferred">What is to be Inferred?</h1>
            <p>Explain what we aim to infer in statistics, using clear,
            non-technical language. Discuss concepts like population mean, mode,
            proportion, standard deviation, and comparisons of means. Use real-life
            examples to describe these concepts without relying on complex
            mathematical jargon.</p>
            <h1 id="how-to-infer">How to Infer?</h1>
            <p>mention also three important distribution anfnd later make connection
            to method of inference Discuss the process of making inferences by using
            sample data. Emphasize the importance of sampling and why it matters.
            Introduce foundational concepts like the Central Limit Theorem and the
            Law of Large Numbers, explaining how they provide the theoretical basis
            for statistical inference. Include examples of hypothesis testing and
            types of statistical tests (e.g., t-tests, chi-square tests) in a
            conceptual manner.</p>
            <h1 id="how-confident-are-we-about-our-inference">How Confident Are We
            About Our Inference?</h1>
            <p>Introduce the idea of confidence intervals and how they help us
            measure the reliability of our estimates. Explain the concept of
            hypothesis testing, including the role of errors (Type I and Type II
            errors) in statistical tests. Provide conceptual explanations without
            getting too technical.</p>
            <h1 id="common-pitfalls">Common Pitfalls</h1>
            <p>Statistical inference can be tricky for newcomers, and it’s easy to
            fall into a few common pitfalls. Let’s highlight some of these mistakes
            and misconceptions to help you avoid them:</p>
            <ol>
            <li><p><strong>Confusing Correlation with Causation</strong> One of the
            most frequent errors in data analysis is assuming that correlation
            implies causation. Just because two variables move together doesn’t mean
            one causes the other. For example, if sales of ice cream increase during
            hot weather, it doesn’t mean that ice cream causes hot weather! Always
            be cautious about interpreting relationships — correlation shows
            association, not causation.</p></li>
            <li><p><strong>Misunderstanding the p-value</strong> Many beginners
            believe that a p-value is the probability that the null hypothesis is
            true. This is incorrect. The p-value measures how likely it is to
            observe your data (or something more extreme) if the null hypothesis
            were true. For example, a p-value of 0.03 doesn’t mean there’s a 3%
            chance the null hypothesis is true — it means that, if the null
            hypothesis were true, there’s a 3% chance of seeing data as extreme as
            what you observed.</p></li>
            <li><p><strong>Relying Too Much on Small Sample Sizes</strong></p>
            <p>Small samples can lead to misleading conclusions. A common mistake is
            thinking that a small sample will always represent the population
            accurately. In reality, smaller samples are much more likely to show
            random variations that don’t reflect the true population
            characteristics. Larger sample sizes generally provide more reliable and
            stable results.</p></li>
            <li><p><strong>Ignoring Assumptions of Statistical Tests</strong></p>
            <p>Many statistical tests have underlying assumptions — such as
            normality of data, equal variances, or independence of observations.
            Ignoring these assumptions can lead to inaccurate conclusions. For
            example, using a t-test when your data is not normally distributed or
            has outliers can distort your results. Always check that your data meets
            the assumptions of the test you’re using.</p></li>
            <li><p><strong>Over-Interpreting Confidence Intervals</strong></p>
            <p>Confidence intervals are useful tools, but they can be
            misinterpreted. A common mistake is to think that a 95% confidence
            interval means there’s a 95% chance the true parameter lies within the
            interval. In fact, it means that if you were to repeat the sampling
            process many times, 95% of those intervals would contain the true
            parameter — but for any given interval, the parameter either is or isn’t
            within it.</p></li>
            <li><p><strong>Cherry-Picking Data</strong></p>
            <p>It’s tempting to choose only the data that supports your hypothesis
            or business goal. However, cherry-picking data or ignoring contradictory
            information can lead to biased conclusions and flawed decisions. It’s
            important to take a holistic view of all the data and remain
            objective.</p></li>
            <li><p><strong>Misusing Statistical Significance</strong></p>
            <p>Just because something is statistically significant doesn’t
            necessarily mean it’s practically important. A result might show
            statistical significance (e.g., a p-value &lt; 0.05), but the actual
            effect size could be so small that it’s irrelevant in the real world.
            Always consider the magnitude of the effect alongside statistical
            significance.</p></li>
            </ol>
            <h1 id="steps-from-data-to-decision">Steps from Data to Decision</h1>
            <p>In practice, statistical inference follows a series of structured
            steps that take us from the raw data collection stage to making
            data-driven decisions. Below is an outline of these steps, presented as
            an algorithm for conducting statistical inference.</p>
            <h2 class="unnumbered"
            id="algorithm-steps-for-statistical-inference-in-practice">Algorithm:
            Steps for Statistical Inference in Practice</h2>
            <ol>
            <li><p><strong>Define the Problem and Research Question</strong></p>
            <ul>
            <li><p>Clearly articulate the problem you are trying to solve or the
            question you are investigating.</p></li>
            <li><p><strong>Example:</strong> "Is the new drug more effective at
            lowering blood pressure than the standard treatment?" or "Do college
            graduates earn more than non-graduates?"</p></li>
            </ul></li>
            <li><p><strong>Collect a Representative Sample of Data</strong></p>
            <ul>
            <li><p>Design the data collection process carefully, ensuring the sample
            is random and representative of the population. The quality of the
            sample is crucial for making valid inferences.</p></li>
            <li><p><strong>Example:</strong> Survey 500 randomly selected households
            for income data, or conduct a controlled clinical trial for the new
            drug.</p></li>
            </ul></li>
            <li><p><strong>Summarize and Explore the Data</strong></p>
            <ul>
            <li><p>Organize the data in tables, graphs, and descriptive statistics
            (mean, median, standard deviation, etc.).</p></li>
            <li><p>Explore patterns, trends, or anomalies in the data to get a
            preliminary understanding.</p></li>
            <li><p><strong>Example:</strong> For income data, calculate the mean and
            median incomes of the sample and look for any outliers or extreme
            values.</p></li>
            </ul></li>
            <li><p><strong>Formulate the Hypothesis</strong></p>
            <ul>
            <li><p>Establish the <strong>null hypothesis (H<span
            class="math inline"><sub>0</sub></span>)</strong> and the
            <strong>alternative hypothesis (H<span
            class="math inline"><sub>1</sub></span>)</strong> based on the
            problem.</p>
            <ul>
            <li><p><strong>H<span class="math inline"><sub>0</sub></span></strong>:
            There is no effect or difference.</p></li>
            <li><p><strong>H<span class="math inline"><sub>1</sub></span></strong>:
            There is an effect or a difference.</p></li>
            </ul></li>
            <li><p><strong>Example:</strong> For the new drug:</p>
            <ul>
            <li><p><strong>H<span class="math inline"><sub>0</sub></span></strong>:
            The new drug’s effect is the same as the standard treatment.</p></li>
            <li><p><strong>H<span class="math inline"><sub>1</sub></span></strong>:
            The new drug is more effective.</p></li>
            </ul></li>
            </ul></li>
            <li><p><strong>Choose the Appropriate Statistical Test</strong></p>
            <ul>
            <li><p>Select the correct test based on the type of data and the
            research question. Common tests include:</p>
            <ul>
            <li><p><strong>t-test</strong> for comparing means.</p></li>
            <li><p><strong>Chi-square test</strong> for categorical data.</p></li>
            <li><p><strong>ANOVA</strong> for comparing multiple groups.</p></li>
            <li><p><strong>Regression analysis</strong> for predicting relationships
            between variables.</p></li>
            </ul></li>
            <li><p><strong>Example:</strong> For comparing blood pressure reductions
            between two drugs, you might use a t-test for independent
            samples.</p></li>
            </ul></li>
            <li><p><strong>Check Assumptions</strong></p>
            <ul>
            <li><p>Verify the assumptions behind the statistical test (e.g., normal
            distribution of data, equal variances). If these assumptions do not
            hold, consider alternative methods or data transformations.</p></li>
            <li><p><strong>Example:</strong> Check whether blood pressure reductions
            are normally distributed and if variances are similar between the two
            groups.</p></li>
            </ul></li>
            <li><p><strong>Compute the Test Statistic and p-Value</strong></p>
            <ul>
            <li><p>Perform the statistical test to calculate the test statistic
            (e.g., t-value, F-value, etc.).</p></li>
            <li><p>Calculate the <strong>p-value</strong>, which measures the
            probability that the observed effect could have occurred by chance under
            the null hypothesis.</p></li>
            <li><p><strong>Example:</strong> Compute the t-value for the difference
            in average blood pressure reduction, and find the associated
            p-value.</p></li>
            </ul></li>
            <li><p><strong>Make the Decision</strong></p>
            <ul>
            <li><p>Compare the p-value to a predefined significance level (usually
            <span class="math inline"><em>α</em> = 0.05</span>):</p>
            <ul>
            <li><p>If <span class="math inline"><em>p</em> ≤ <em>α</em></span>,
            reject the null hypothesis (<strong>H<span
            class="math inline"><sub>0</sub></span></strong>) and conclude that
            there is a statistically significant effect.</p></li>
            <li><p>If <span class="math inline"><em>p</em> &gt; <em>α</em></span>,
            fail to reject the null hypothesis and conclude that the observed effect
            may have occurred by chance.</p></li>
            </ul></li>
            <li><p><strong>Example:</strong> If the p-value for the blood pressure
            test is less than 0.05, conclude that the new drug is statistically more
            effective than the standard treatment.</p></li>
            </ul></li>
            <li><p><strong>Quantify the Uncertainty (Confidence
            Interval)</strong></p>
            <ul>
            <li><p>Calculate the <strong>confidence interval (CI)</strong> to
            quantify the uncertainty around your estimate. A 95% confidence interval
            means you are 95% confident that the true parameter lies within this
            range.</p></li>
            <li><p><strong>Example:</strong> If the difference in blood pressure
            reduction is estimated to be 3 mmHg, the 95% confidence interval might
            be [1.5, 4.5] mmHg.</p></li>
            </ul></li>
            <li><p><strong>Draw Conclusions and Make Decisions</strong></p>
            <ul>
            <li><p>Use the results of the statistical test and the confidence
            intervals to make data-driven decisions or recommendations.</p></li>
            <li><p><strong>Example:</strong> If the new drug shows statistically
            significant and clinically meaningful improvements in lowering blood
            pressure, consider recommending it over the standard treatment.</p></li>
            </ul></li>
            <li><p><strong>Report and Communicate Findings</strong></p>
            <ul>
            <li><p>Present the results in a clear and understandable format,
            including key statistics, test results, and confidence
            intervals.</p></li>
            <li><p>Explain the implications of the results for the decision-making
            process.</p></li>
            <li><p><strong>Example:</strong> Present findings to a medical board or
            company leadership, recommending the new drug based on the
            evidence.</p></li>
            </ul></li>
            </ol>
            <h1 id="problem-and-stepwise-solution">Problem and Stepwise Solution
            </h1>
            <h2 id="problem-does-the-new-weight-loss-program-work">Problem: Does the
            New Weight Loss Program Work?</h2>
            <p>Suppose a fitness company has introduced a new weight loss program.
            The company claims that participants lose an average of 5 kilograms more
            than with the current program. To test this claim, the company conducts
            an experiment with 40 people: 20 use the new program, and 20 use the
            current program. After 8 weeks, the results are as follows:</p>
            <ul>
            <li><p><strong>New program:</strong> mean weight loss = 7 kg, standard
            deviation = 2 kg</p></li>
            <li><p><strong>Current program:</strong> mean weight loss = 5 kg,
            standard deviation = 1.5 kg</p></li>
            </ul>
            <p><strong>Objective:</strong> We want to determine whether the
            difference in weight loss between the two programs is statistically
            significant or if it could have happened by chance.</p>
            <h1 class="unnumbered" id="step-by-step-solution">Step-by-Step
            Solution</h1>
            <h2 class="unnumbered" id="step-1-state-the-hypotheses">Step 1: State
            the Hypotheses</h2>
            <p>We establish the null and alternative hypotheses.</p>
            <ul>
            <li><p><strong>Null Hypothesis (<span
            class="math inline"><em>H</em><sub>0</sub></span>):</strong> There is no
            difference in the mean weight loss between the two programs. <span
            class="math display"><em>H</em><sub>0</sub> : <em>μ</em><sub>new</sub> = <em>μ</em><sub>current</sub></span></p></li>
            <li><p><strong>Alternative Hypothesis (<span
            class="math inline"><em>H</em><sub>1</sub></span>):</strong> The new
            program leads to greater weight loss. <span
            class="math display"><em>H</em><sub>1</sub> : <em>μ</em><sub>new</sub> &gt; <em>μ</em><sub>current</sub></span></p></li>
            </ul>
            <h2 class="unnumbered"
            id="step-2-choose-the-significance-level-alpha">Step 2: Choose the
            Significance Level (<span class="math inline"><em>α</em></span>)</h2>
            <p>We choose a significance level of <span
            class="math inline"><em>α</em> = 0.05</span>.</p>
            <h2 class="unnumbered" id="step-3-gather-the-data">Step 3: Gather the
            Data</h2>
            <p>We are given the following data:</p>
            <ul>
            <li><p><strong>New program:</strong> <span
            class="math display"><em>X̄</em><sub>new</sub> = 7 kg, <em>S</em><sub>new</sub> = 2 kg, <em>n</em><sub>new</sub> = 20</span></p></li>
            <li><p><strong>Current program:</strong> <span
            class="math display"><em>X̄</em><sub>current</sub> = 5 kg, <em>S</em><sub>current</sub> = 1.5 kg, <em>n</em><sub>current</sub> = 20</span></p></li>
            </ul>
            <h2 class="unnumbered" id="step-4-perform-the-calculation">Step 4:
            Perform the Calculation</h2>
            <p><strong>1. Calculate the Standard Error (SE) for the difference
            between means:</strong></p>
            <p><span class="math display">$$SE =
            \sqrt{\left(\frac{S_{\text{new}}^2}{n_{\text{new}}}\right) +
            \left(\frac{S_{\text{current}}^2}{n_{\text{current}}}\right)}$$</span></p>
            <p>Substitute the values: <span class="math display">$$SE =
            \sqrt{\left(\frac{2^2}{20}\right) + \left(\frac{1.5^2}{20}\right)} =
            \sqrt{\left(\frac{4}{20}\right) + \left(\frac{2.25}{20}\right)} =
            \sqrt{0.2 + 0.1125} = \sqrt{0.3125} = 0.559$$</span></p>
            <p><strong>2. Compute the Test Statistic (t-value):</strong></p>
            <p>The t-value is calculated as: <span class="math display">$$t =
            \frac{(\bar{X}_{\text{new}} -
            \bar{X}_{\text{current}})}{SE}$$</span></p>
            <p>Substitute the values: <span class="math display">$$t = \frac{(7 -
            5)}{0.559} = \frac{2}{0.559} = 3.58$$</span></p>
            <h2 class="unnumbered" id="step-5-determine-the-p-value">Step 5:
            Determine the p-value</h2>
            <p>Using a t-distribution table or calculator, the p-value corresponding
            to <span class="math inline"><em>t</em> = 3.58</span> with 38 degrees of
            freedom is approximately: <span
            class="math display"><em>p</em> = 0.0004</span></p>
            <h2 class="unnumbered" id="step-6-make-the-decision">Step 6: Make the
            Decision</h2>
            <p>Compare the p-value to the significance level <span
            class="math inline"><em>α</em> = 0.05</span>:</p>
            <ul>
            <li><p>If <span class="math inline"><em>p</em> ≤ <em>α</em></span>,
            reject the null hypothesis <span
            class="math inline"><em>H</em><sub>0</sub></span>.</p></li>
            <li><p>If <span class="math inline"><em>p</em> &gt; <em>α</em></span>,
            fail to reject <span
            class="math inline"><em>H</em><sub>0</sub></span>.</p></li>
            </ul>
            <p>In this case, <span class="math inline"><em>p</em> = 0.0004</span>,
            which is less than <span class="math inline"><em>α</em> = 0.05</span>,
            so we <strong>reject the null hypothesis</strong>. This suggests that
            the new weight loss program leads to significantly greater weight loss
            than the current program.</p>
            <h2 class="unnumbered" id="step-7-conclusion">Step 7: Conclusion</h2>
            <p>Based on the statistical test, the new weight loss program leads to
            more weight loss than the current program. With a t-value of 3.58 and a
            p-value of 0.0004, we can confidently conclude that the observed
            difference in weight loss is statistically significant and not due to
            random chance.</p>
            <h2 id="problem-two-population-testing-independent-t-test">Problem:
            Two-Population Testing (Independent t-test)</h2>
            <p>A school is testing two different teaching methods to determine which
            one leads to better student performance. The school randomly assigns 30
            students to each method. After 12 weeks, the results (mean test scores)
            are as follows:</p>
            <ul>
            <li><p><strong>Method A</strong>: Mean test score = 85, Standard
            deviation = 5, Sample size = 30</p></li>
            <li><p><strong>Method B</strong>: Mean test score = 80, Standard
            deviation = 6, Sample size = 30</p></li>
            </ul>
            <p><strong>Objective:</strong> Determine whether the difference in test
            scores between the two methods is statistically significant.</p>
            <h1 class="unnumbered" id="step-by-step-solution-1">Step-by-Step
            Solution</h1>
            <h2 class="unnumbered" id="step-1-state-the-hypotheses-1">Step 1: State
            the Hypotheses</h2>
            <ul>
            <li><p><strong>Null Hypothesis (<span
            class="math inline"><em>H</em><sub>0</sub></span>)</strong>: There is no
            difference in the mean test scores between the two teaching methods.
            <span
            class="math display"><em>H</em><sub>0</sub> : <em>μ</em><sub><em>A</em></sub> = <em>μ</em><sub><em>B</em></sub></span></p></li>
            <li><p><strong>Alternative Hypothesis (<span
            class="math inline"><em>H</em><sub>1</sub></span>)</strong>: Method A
            leads to higher test scores than Method B. <span
            class="math display"><em>H</em><sub>1</sub> : <em>μ</em><sub><em>A</em></sub> &gt; <em>μ</em><sub><em>B</em></sub></span></p></li>
            </ul>
            <h2 class="unnumbered"
            id="step-2-choose-the-significance-level-alpha-1">Step 2: Choose the
            Significance Level (<span class="math inline"><em>α</em></span>)</h2>
            <p>We choose a significance level of <span
            class="math inline"><em>α</em> = 0.05</span>.</p>
            <h2 class="unnumbered" id="step-3-gather-the-data-1">Step 3: Gather the
            Data</h2>
            <p>The provided data for each teaching method is as follows:</p>
            <ul>
            <li><p><strong>Method A:</strong> <span
            class="math inline"><em>X̄</em><sub><em>A</em></sub> = 85, <em>S</em><sub><em>A</em></sub> = 5, <em>n</em><sub><em>A</em></sub> = 30</span></p></li>
            <li><p><strong>Method B:</strong> <span
            class="math inline"><em>X̄</em><sub><em>B</em></sub> = 80, <em>S</em><sub><em>B</em></sub> = 6, <em>n</em><sub><em>B</em></sub> = 30</span></p></li>
            </ul>
            <h2 class="unnumbered"
            id="step-4-conduct-the-independent-two-sample-t-test">Step 4: Conduct
            the Independent Two-Sample t-test</h2>
            <p>We use the following formula to compute the t-statistic for
            independent samples:</p>
            <p><span class="math display">$$t = \frac{\bar{X}_A -
            \bar{X}_B}{\sqrt{\frac{S_A^2}{n_A} + \frac{S_B^2}{n_B}}}$$</span></p>
            <p>Substituting the values: <span class="math display">$$t = \frac{85 -
            80}{\sqrt{\frac{5^2}{30} + \frac{6^2}{30}}} =
            \frac{5}{\sqrt{\frac{25}{30} + \frac{36}{30}}} = \frac{5}{\sqrt{0.833 +
            1.2}} = \frac{5}{\sqrt{2.033}} = \frac{5}{1.426} \approx
            3.51$$</span></p>
            <h2 class="unnumbered"
            id="step-5-determine-the-degrees-of-freedom-df">Step 5: Determine the
            Degrees of Freedom (df)</h2>
            <p>The degrees of freedom for an independent t-test are approximated
            using the following formula: <span class="math display">$$df =
            \frac{\left( \frac{S_A^2}{n_A} + \frac{S_B^2}{n_B}
            \right)^2}{\frac{\left( \frac{S_A^2}{n_A} \right)^2}{n_A - 1} +
            \frac{\left( \frac{S_B^2}{n_B} \right)^2}{n_B - 1}}$$</span>
            Substituting the values: <span class="math display">$$df = \frac{(0.833
            + 1.2)^2}{\frac{(0.833)^2}{29} + \frac{(1.2)^2}{29}} =
            \frac{(2.033)^2}{\frac{0.694}{29} + \frac{1.44}{29}} =
            \frac{4.133}{\frac{0.694 + 1.44}{29}} = \frac{4.133}{0.0735} \approx
            56.22$$</span> So the degrees of freedom are approximately <span
            class="math inline"><em>d</em><em>f</em> = 56</span>.</p>
            <h2 class="unnumbered"
            id="step-6-compare-the-t-statistic-to-the-critical-value">Step 6:
            Compare the t-Statistic to the Critical Value</h2>
            <p>Using a t-distribution table, the critical value for a one-tailed
            test at <span class="math inline"><em>α</em> = 0.05</span> and <span
            class="math inline"><em>d</em><em>f</em> = 56</span> is approximately
            <span
            class="math inline"><em>t</em><sub><em>c</em><em>r</em><em>i</em><em>t</em><em>i</em><em>c</em><em>a</em><em>l</em></sub> = 1.67</span>.</p>
            <p>Since the computed <span class="math inline"><em>t</em> = 3.51</span>
            is greater than <span
            class="math inline"><em>t</em><sub><em>c</em><em>r</em><em>i</em><em>t</em><em>i</em><em>c</em><em>a</em><em>l</em></sub> = 1.67</span>,
            we reject the null hypothesis.</p>
            <h2 class="unnumbered" id="step-7-conclusion-1">Step 7: Conclusion</h2>
            <p>The test provides sufficient evidence to conclude that Method A leads
            to significantly higher test scores than Method B.</p>
            <h2 id="problem-chi-square-test-for-independence">Problem: Chi-Square
            Test for Independence</h2>
            <p><strong>Problem:</strong> Is There a Relationship Between Exercise
            and Sleep Quality?<br />
            A health study examines whether exercise frequency is related to sleep
            quality. The data is collected from 100 individuals and presented in the
            following contingency table:</p>
            <table>
            <caption>Contingency Table for Exercise and Sleep Quality</caption>
            <thead>
            <tr class="header">
            <th style="text-align: center;"></th>
            <th style="text-align: center;"><strong>Good Sleep Quality</strong></th>
            <th style="text-align: center;"><strong>Poor Sleep Quality</strong></th>
            <th style="text-align: center;"><strong>Total</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td style="text-align: center;"><strong>Exercises</strong></td>
            <td style="text-align: center;">30</td>
            <td style="text-align: center;">10</td>
            <td style="text-align: center;">40</td>
            </tr>
            <tr class="even">
            <td style="text-align: center;"><strong>Does Not Exercise</strong></td>
            <td style="text-align: center;">20</td>
            <td style="text-align: center;">40</td>
            <td style="text-align: center;">60</td>
            </tr>
            <tr class="odd">
            <td style="text-align: center;"><strong>Total</strong></td>
            <td style="text-align: center;">50</td>
            <td style="text-align: center;">50</td>
            <td style="text-align: center;">100</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Objective:</strong> Determine whether there is a relationship
            between exercise and sleep quality.</p>
            <h2 class="unnumbered" id="step-by-step-solution-2">Step-by-Step
            Solution</h2>
            <p><strong>Step 1: State the Hypotheses</strong></p>
            <ul>
            <li><p><strong>Null Hypothesis (<span
            class="math inline"><em>H</em><sub>0</sub></span>):</strong> Exercise
            and sleep quality are independent (no relationship). <span
            class="math display"><em>H</em><sub>0</sub> : Exercise is independent of
            sleep quality</span></p></li>
            <li><p><strong>Alternative Hypothesis (<span
            class="math inline"><em>H</em><sub>1</sub></span>):</strong> There is a
            relationship between exercise and sleep quality. <span
            class="math display"><em>H</em><sub>1</sub> : Exercise is not
            independent of sleep quality</span></p></li>
            </ul>
            <p><strong>Step 2: Choose the Significance Level (<span
            class="math inline"><em>α</em></span>)</strong><br />
            We choose <span class="math inline"><em>α</em> = 0.05</span>.</p>
            <p><strong>Step 3: Calculate Expected Frequencies</strong></p>
            <p>First, we calculate the expected frequencies based on the marginal
            totals in the contingency table. The formula to compute the expected
            frequency for each cell is: <span class="math display">$$E_{ij} =
            \frac{(\text{Row Total}) \times (\text{Column Total})}{\text{Grand
            Total}}$$</span></p>
            <p><strong>Expected Frequencies:</strong></p>
            <ul>
            <li><p>For <strong>Exercises</strong> and <strong>Good Sleep
            Quality</strong>: <span class="math display">$$E = \frac{(40) \times
            (50)}{100} = 20$$</span></p></li>
            <li><p>For <strong>Exercises</strong> and <strong>Poor Sleep
            Quality</strong>: <span class="math display">$$E = \frac{(40) \times
            (50)}{100} = 20$$</span></p></li>
            <li><p>For <strong>Does Not Exercise</strong> and <strong>Good Sleep
            Quality</strong>: <span class="math display">$$E = \frac{(60) \times
            (50)}{100} = 30$$</span></p></li>
            <li><p>For <strong>Does Not Exercise</strong> and <strong>Poor Sleep
            Quality</strong>: <span class="math display">$$E = \frac{(60) \times
            (50)}{100} = 30$$</span></p></li>
            </ul>
            <p>The expected frequencies are as follows:</p>
            <table>
            <caption>Expected Frequencies for Exercise and Sleep Quality</caption>
            <thead>
            <tr class="header">
            <th style="text-align: center;"></th>
            <th style="text-align: center;"><strong>Good Sleep Quality</strong></th>
            <th style="text-align: center;"><strong>Poor Sleep Quality</strong></th>
            <th style="text-align: center;"><strong>Total</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td style="text-align: center;"><strong>Exercises
            (Expected)</strong></td>
            <td style="text-align: center;">20</td>
            <td style="text-align: center;">20</td>
            <td style="text-align: center;">40</td>
            </tr>
            <tr class="even">
            <td style="text-align: center;"><strong>Does Not Exercise
            (Expected)</strong></td>
            <td style="text-align: center;">30</td>
            <td style="text-align: center;">30</td>
            <td style="text-align: center;">60</td>
            </tr>
            <tr class="odd">
            <td style="text-align: center;"><strong>Total</strong></td>
            <td style="text-align: center;">50</td>
            <td style="text-align: center;">50</td>
            <td style="text-align: center;">100</td>
            </tr>
            </tbody>
            </table>
            <p><strong>Step 4: Conduct the Chi-Square Test</strong><br />
            Using the observed and expected frequencies, we calculate the chi-square
            statistic using the formula: <span class="math display">$$\chi^2 = \sum
            \frac{(O_i - E_i)^2}{E_i}$$</span></p>
            <p><strong>Observed Values:</strong> <span
            class="math inline"><em>O</em> = {30, 10, 20, 40}</span></p>
            <p><strong>Expected Values:</strong> <span
            class="math inline"><em>E</em> = {20, 20, 30, 30}</span></p>
            <p>Now, calculate <span
            class="math inline"><em>χ</em><sup>2</sup></span>: <span
            class="math display">$$\chi^2 = \frac{(30 - 20)^2}{20} + \frac{(10 -
            20)^2}{20} + \frac{(20 - 30)^2}{30} + \frac{(40 - 30)^2}{30}$$</span>
            <span class="math display">$$\chi^2 = \frac{(10)^2}{20} +
            \frac{(-10)^2}{20} + \frac{(-10)^2}{30} + \frac{(10)^2}{30}$$</span>
            <span class="math display">$$\chi^2 = \frac{100}{20} + \frac{100}{20} +
            \frac{100}{30} + \frac{100}{30}$$</span> <span
            class="math display"><em>χ</em><sup>2</sup> = 5 + 5 + 3.33 + 3.33 = 16.66</span></p>
            <p><strong>Step 5: Compare the Chi-Square Statistic to the Critical
            Value</strong></p>
            <p>We have <span class="math inline">1</span> degree of freedom (<span
            class="math inline"><em>d</em><em>f</em> = (<em>r</em>−1)(<em>c</em>−1) = 1</span>).</p>
            <p>From the chi-square distribution table, the critical value for <span
            class="math inline"><em>α</em> = 0.05</span> and <span
            class="math inline">1</span> degree of freedom is <span
            class="math inline">3.84</span>.</p>
            <p>Since <span class="math inline"><em>χ</em><sup>2</sup> = 16.66</span>
            is greater than <span class="math inline">3.84</span>, we reject the
            null hypothesis.</p>
            <p><strong>Step 6: Conclusion</strong></p>
            <p>There is sufficient evidence to conclude that exercise frequency is
            related to sleep quality.</p>
            <h2 id="problem-one-way-anova">Problem: One-Way ANOVA</h2>
            <p><strong>Problem:</strong> Do Different Fertilizers Affect Plant
            Growth?<br />
            A botanist tests whether three different fertilizers lead to different
            levels of plant growth. She applies Fertilizer A, B, and C to three
            groups of plants. The average growth (in cm) after one month is as
            follows:</p>
            <ul>
            <li><p><strong>Fertilizer A:</strong> Growth = 10 cm, 12 cm, 11
            cm</p></li>
            <li><p><strong>Fertilizer B:</strong> Growth = 14 cm, 15 cm, 16
            cm</p></li>
            <li><p><strong>Fertilizer C:</strong> Growth = 9 cm, 8 cm, 10
            cm</p></li>
            </ul>
            <p><strong>Objective:</strong> Determine whether there is a significant
            difference in plant growth between the three fertilizers.</p>
            <h2 class="unnumbered" id="step-by-step-solution-3">Step-by-Step
            Solution</h2>
            <p><strong>Step 1: State the Hypotheses</strong></p>
            <ul>
            <li><p><strong>Null Hypothesis (<span
            class="math inline"><em>H</em><sub>0</sub></span>):</strong> All three
            fertilizers lead to the same average growth. <span
            class="math display"><em>H</em><sub>0</sub> : <em>μ</em><sub><em>A</em></sub> = <em>μ</em><sub><em>B</em></sub> = <em>μ</em><sub><em>C</em></sub></span></p></li>
            <li><p><strong>Alternative Hypothesis (<span
            class="math inline"><em>H</em><sub>1</sub></span>):</strong> At least
            one fertilizer leads to different growth. <span
            class="math display"><em>H</em><sub>1</sub> : At least one<em>μ</em> is
            different</span></p></li>
            </ul>
            <p><strong>Step 2: Choose the Significance Level (<span
            class="math inline"><em>α</em></span>)</strong><br />
            We choose <span class="math inline"><em>α</em> = 0.05</span>.</p>
            <p><strong>Step 3: Conduct the ANOVA</strong></p>
            <p>In one-way ANOVA, we decompose the total variation into two
            components:</p>
            <ul>
            <li><p><strong>Between-group variation:</strong> How much the group
            means vary from the overall mean.</p></li>
            <li><p><strong>Within-group variation:</strong> How much the individual
            values vary within each group.</p></li>
            </ul>
            <h3 class="unnumbered"
            id="step-3.1-compute-the-group-means-and-the-overall-mean">Step 3.1:
            Compute the Group Means and the Overall Mean</h3>
            <p><span class="math display">$$\text{Group means:} \quad \bar{X}_A =
            \frac{10 + 12 + 11}{3} = 11, \quad \bar{X}_B = \frac{14 + 15 + 16}{3} =
            15, \quad \bar{X}_C = \frac{9 + 8 + 10}{3} = 9$$</span> <span
            class="math display">$$\text{Overall mean:} \quad \bar{X} = \frac{11 +
            15 + 9}{3} = 11.67$$</span></p>
            <h3 class="unnumbered"
            id="step-3.2-compute-the-between-group-sum-of-squares-ssb">Step 3.2:
            Compute the Between-Group Sum of Squares (SSB)</h3>
            <p>The formula for the between-group sum of squares is: <span
            class="math display"><em>S</em><em>S</em><em>B</em> = <em>n</em><sub><em>A</em></sub>(<em>X̄</em><sub><em>A</em></sub>−<em>X̄</em>)<sup>2</sup> + <em>n</em><sub><em>B</em></sub>(<em>X̄</em><sub><em>B</em></sub>−<em>X̄</em>)<sup>2</sup> + <em>n</em><sub><em>C</em></sub>(<em>X̄</em><sub><em>C</em></sub>−<em>X̄</em>)<sup>2</sup></span>
            Where <span
            class="math inline"><em>n</em><sub><em>A</em></sub> = <em>n</em><sub><em>B</em></sub> = <em>n</em><sub><em>C</em></sub> = 3</span>
            (each group has 3 observations). <span
            class="math display"><em>S</em><em>S</em><em>B</em> = 3(11−11.67)<sup>2</sup> + 3(15−11.67)<sup>2</sup> + 3(9−11.67)<sup>2</sup> = 3(0.67<sup>2</sup>) + 3(3.33<sup>2</sup>) + 3(2.67<sup>2</sup>)</span>
            <span
            class="math display"><em>S</em><em>S</em><em>B</em> = 3(0.4489) + 3(11.0889) + 3(7.1289) = 1.3467 + 33.2667 + 21.3867 = 56</span></p>
            <h3 class="unnumbered"
            id="step-3.3-compute-the-within-group-sum-of-squares-ssw">Step 3.3:
            Compute the Within-Group Sum of Squares (SSW)</h3>
            <p>The formula for the within-group sum of squares is: <span
            class="math display"><em>S</em><em>S</em><em>W</em> = ∑(Individual
            value−Group mean)<sup>2</sup></span></p>
            <p>For Fertilizer A: <span
            class="math display"><em>S</em><em>S</em><em>W</em><sub><em>A</em></sub> = (10−11)<sup>2</sup> + (12−11)<sup>2</sup> + (11−11)<sup>2</sup> = 1 + 1 + 0 = 2</span></p>
            <p>For Fertilizer B: <span
            class="math display"><em>S</em><em>S</em><em>W</em><sub><em>B</em></sub> = (14−15)<sup>2</sup> + (15−15)<sup>2</sup> + (16−15)<sup>2</sup> = 1 + 0 + 1 = 2</span></p>
            <p>For Fertilizer C: <span
            class="math display"><em>S</em><em>S</em><em>W</em><sub><em>C</em></sub> = (9−9)<sup>2</sup> + (8−9)<sup>2</sup> + (10−9)<sup>2</sup> = 0 + 1 + 1 = 2</span></p>
            <p><span
            class="math display"><em>S</em><em>S</em><em>W</em> = <em>S</em><em>S</em><em>W</em><sub><em>A</em></sub> + <em>S</em><em>S</em><em>W</em><sub><em>B</em></sub> + <em>S</em><em>S</em><em>W</em><sub><em>C</em></sub> = 2 + 2 + 2 = 6</span></p>
            <h3 class="unnumbered" id="step-3.4-compute-the-degrees-of-freedom">Step
            3.4: Compute the Degrees of Freedom</h3>
            <p><span class="math display">Degrees of freedom between groups
            (dfB) = <em>k</em> − 1 = 3 − 1 = 2</span> <span
            class="math display">Degrees of freedom within groups
            (dfW) = <em>N</em> − <em>k</em> = 9 − 3 = 6</span></p>
            <h3 class="unnumbered" id="step-3.5-compute-the-mean-squares">Step 3.5:
            Compute the Mean Squares</h3>
            <p><span class="math display">$$\text{Mean square between groups (MSB)}
            = \frac{SSB}{dfB} = \frac{56}{2} = 28$$</span> <span
            class="math display">$$\text{Mean square within groups (MSW)} =
            \frac{SSW}{dfW} = \frac{6}{6} = 1$$</span></p>
            <h3 class="unnumbered" id="step-3.6-compute-the-f-statistic">Step 3.6:
            Compute the F-Statistic</h3>
            <p><span class="math display">$$F = \frac{MSB}{MSW} = \frac{28}{1} =
            28$$</span></p>
            <h3 class="unnumbered"
            id="step-3.7-compare-the-f-statistic-to-the-critical-value">Step 3.7:
            Compare the F-Statistic to the Critical Value</h3>
            <p>We compare the calculated F-statistic to the critical value of <span
            class="math inline"><em>F</em></span> for <span
            class="math inline"><em>d</em><em>f</em><em>B</em> = 2</span> and <span
            class="math inline"><em>d</em><em>f</em><em>W</em> = 6</span> at <span
            class="math inline"><em>α</em> = 0.05</span>. From the F-distribution
            table, the critical value is 5.14. Since <span
            class="math inline"><em>F</em> = 28 &gt; 5.14</span>, we reject the null
            hypothesis.</p>
            <p><strong>Conclusion:</strong> There is sufficient evidence to conclude
            that at least one fertilizer leads to significantly different plant
            growth.</p>
          </div>
            
            </section>



       
            
    <footer>
        <p>&copy; 2024 Statistics Blog | Designed by Janmajay</p>
    </footer>

</body>





</html>
